from typing import Iterator, Optional, Union, Callable

import numpy as np

from .typing import DrawAndLogP, GradModel, Seed, VectorType


class DrGhmcDiag:
    """Generalized HMC sampler with (Probabilistic) Delayed Rejection diagonal metric.

    To sample from a target distribution, DrGhmcDiag proposes a new sample from the
    current sample using the leapfrog integrator. The probability of accepting this
    transition is recursively computed. If accepted, DrGhmcDiag returns the proposed
    sample and its log density. If rejected, DrGhmcDiag proposes a new sample generated
    with new stepsize and number of steps for the leapfrog integrator. This process is
    repeated until a maximum number of proposals is reached or probabilistic delayed
    rejection terminates early.

    Intuitively, DrGhmcDiag reduces the stepsize in subsequent proposals if the
    previous proposal was rejected for better sampling resolution of high-curvature
    state space regions.

    Implementation based on Modi, Chirag, Alex Barnett, and Bob Carpenter. "Delayed
    rejection Hamiltonian Monte Carlo for sampling multiscale distributions." Bayesian
    Analysis 1.1 (2023): 1-28. While the paper represents position and momentum with
    (p, q), this implementation uses (theta, rho) for consistency with bayes-kit. All
    computation is done on the log scale unless explicitly stated otherwise.
    """

    def __init__(
        self,
        model: GradModel,
        stepsize: Union[int | float, list[int | float], Callable[[int], int | float]],
        steps: Union[int, list[int], Callable[[int], int]],
        metric_diag: Optional[VectorType] = None,
        init: Optional[VectorType] = None,
        seed: Optional[Seed] = None,
        num_proposals: int = 3,
        probabilistic: bool = True,
        dampening: float = 1.0,
    ):
        """Initialize the DrGhmcDiag sampler.

        Args:
            model: model with log density and gradient
            stepsize: stepsize in each leapfrog step. Defaults to None.
            steps: number of leapfrog steps. Defaults to None.
            metric_diag: diagonal of the metric. Defaults to None.
            init: initialize Markov chain. Defaults to None.
            seed: seed for numpy rng. Defaults to None.
            num_proposals: number of delayed rejection proposals. Defaults to 3.
            probabilistic: whether to use probabilistic delayed rejection. Defaults to True.
            dampening: GHMC momentum dampening factor in [0, 1] inclusive. Defaults to 1.0.
        """
        self._model = model
        self._dim = self._model.dims()
        self._num_proposals = num_proposals
        self._stepsize_list = self._init_stepsize(stepsize)
        self._steps_list = self._init_steps(steps)
        self._metric = metric_diag or np.ones(self._dim)
        self._rng = np.random.default_rng(seed)
        self._theta = (
            init
            if (init is not None and init.shape != (0,))
            else self._rng.normal(size=self._dim)
        )
        self._rho = self._rng.normal(size=self._dim)
        self._probabilistic = probabilistic
        if not 0 <= dampening <= 1:
            raise ValueError(f"dampening must be within [0, 1] inclusive")
        self._dampening = dampening

    def _init_stepsize(
        self,
        stepsize: Union[int | float, list[int | float], Callable[[int], int | float]],
    ) -> list[float]:
        """List with leapfrog stepsizes used to generate each proposed sample.

        Each proposed sample (theta_prop, rho_prop) is generated by taking multiple
        leapfrog steps with a a fixed stepsize. Each distinct proposal, however, may
        have a different stepsize. This function initializes the stepsize for
        self._num_proposal such proposals.

        This function accepts, among other valid inputs, a callable stepsize function.
        This callable should accept the (integer) proposal number and return the
        leapfrog stepsize for that proposal.

        Args:
            stepsize: stepsize in each leapfrog step

        Args:
            stepsize: done

        Raises:
            ValueError: scalar with non-positive stepsize
            ValueError: list with incorrect number of stepsizes
            ValueError: list with non-positive stepsize
            TypeError: list with non-numeric (int, float) stepsizes
            ValueError: callable returns non-positive stepsize
            TypeError: callable returns non-numeric (int, float) stepsize
            TypeError: invalid type for stepsize

        Returns:
            list[float]: leapfrog stepsizes for self._num_proposal proposals
        """
        if type(stepsize) is int or type(stepsize) is float:
            if not stepsize > 0:
                raise ValueError(
                    f"stepsize must be positive, but found stepsize of {stepsize}"
                )
            return [float(stepsize)] * self._num_proposals

        elif type(stepsize) is list:
            if len(stepsize) != self._num_proposals:
                raise ValueError(
                    f"list of stepsize must be of length {self._num_proposals}, but found {len(stepsize)} stepsizes"
                )
            for s in stepsize:
                if not s > 0:
                    raise ValueError(
                        f"stepsize must be positive, but found stepsize of {s}"
                    )
                if type(s) is not int and type(s) is not float:
                    raise TypeError(
                        f"list of stepsizes must contain integers or floats, but found {type(s)} stepsize"
                    )
            return [float(s) for s in stepsize]

        elif isinstance(stepsize, Callable):  # isinstance() allows for lambda functions
            stepsize = [stepsize(k) for k in range(self._num_proposals)]
            for s in stepsize:
                if not s > 0:
                    raise ValueError(
                        f"stepsize must be positive, but found stepsize of {s}"
                    )
                if type(s) is not int and type(s) is not float:
                    raise TypeError(
                        f"callable must return integer or float stepsize, but found {type(s)} steps"
                    )
            return [float(s) for s in stepsize]

        else:
            raise TypeError("stepsize must be None, int, float, list, or callable")

    def _init_steps(
        self,
        steps: Union[int, list[int], Callable[[int], int]],
    ) -> list[int | float]:
        """List with number of leapfrog steps used to generate each proposed sample.

        Each proposed sample (theta_prop, rho_prop) is generated by taking multiple
        leapfrog steps with a a fixed stepsize. Each distinct proposal, however, may
        have a different number of steps. This function initializes the number of steps
        for self._num_proposal such proposals.

        This function accepts, among other valid inputs, a callable stepsize function.
        This callable should accept the (integer) proposal number and return the
        number of leapfrog steps for that proposal.

        Intuitively, DrGhmcDiag reduces the stepsize in subsequent proposals if the
        previous proposal was rejected for better resolution of high-curvature
        state space regions.

        Args:
            steps: number of leapfrog steps

        Args:
            steps: done

        Raises:
            ValueError: scalar with non-positive number of steps
            ValueError: list with incorrect number of steps
            ValueError: list with non-positive number of steps
            TypeError: list with non-integer number of steps
            ValueError: callable returns non-positive number of steps
            TypeError: callable returns non-integer number of steps
            TypeError: invalid type for steps

        Returns:
            list[int]: number of leapfrog steps for self._num_proposal proposals
        """
        if type(steps) is int:
            if not steps > 0:
                raise ValueError(f"steps must be positive, but found {steps} steps")
            return [steps] * self._num_proposals

        elif type(steps) is list:
            if len(steps) != self._num_proposals:
                raise ValueError(
                    f"list of steps must be of length {self._num_proposals}, but found {len(steps)} steps"
                )
            for s in steps:
                if not s > 0:
                    raise ValueError(f"steps must be positive, but found {s} steps")
                if type(s) is not int:
                    raise TypeError(
                        f"list of steps must contain integers, but found {type(s)} steps"
                    )
            return steps

        elif isinstance(steps, Callable):  # isinstance() allows for lambda functions
            steps = [steps(k) for k in range(self._num_proposals)]
            for s in steps:
                if not s > 0:
                    raise ValueError(f"steps must be positive, but found {s} steps")
                if type(s) is not int:
                    raise TypeError(
                        f"callable must return integers number of steps, but found {type(s)} steps"
                    )
            return steps

        else:
            raise TypeError("steps must be None, int, list, or callable")

    def __iter__(self) -> Iterator[DrawAndLogP]:
        """Use the DrGhmcDiag sampler as an iterator.

        Returns:
            Iterator[DrawAndLogP]: Iterator of draws from DrGhmcDiag sampler
        """
        return self

    def __next__(self) -> DrawAndLogP:
        """Yields next draw from DrGhmcDiag iterator.

        Yields:
            DrawAndLogP: Tuple of (sample, log density)
        """
        return self.sample()

    def joint_logp(self, theta: VectorType, rho: VectorType) -> float:
        """Joint log density of sample (theta, rho) under the cannonical distribution.

        Args:
            theta: position
            rho: momentum

        Returns:
            float: joint log density
        """
        adj: float = 0.5 * np.dot(rho, self._metric * rho)
        return self._model.log_density(theta) - adj

    def leapfrog(
        self,
        theta: VectorType,
        rho: VectorType,
        stepsize: float,
        steps: int,
    ) -> tuple[VectorType, VectorType]:
        """Propose new sample (theta_prop, rho_prop) with leapfrog integrator.

        Discretize and solve Hamilton's equations using the leapfrog integrator to
        deterministically propose a new sample (theta_prop, rho_prop) from the current
        sample (theta, rho).

        Copy theta because numpy's += operator mutates the original array instead of
        creating a new one.

        Initialize rho_mid by going backwards half a step so that the first full-step
        inside the loop brings rho_mid up to +1/2 steps.

        Args:
            theta: position
            rho: momentum
            stepsize: stepsize in each leapfrog step
            steps: number of leapfrog steps

        Returns:
            tuple[VectorType, VectorType]: proposed sample
        """
        theta = np.array(theta, copy=True)
        _, grad = self._model.log_density_gradient(theta)
        rho_mid = rho - 0.5 * stepsize * np.multiply(self._metric, grad).squeeze()
        for _ in range(steps):
            rho_mid += stepsize * np.multiply(self._metric, grad).squeeze()
            theta += stepsize * rho_mid
            _, grad = self._model.log_density_gradient(theta)
        rho = rho_mid + 0.5 * stepsize * np.multiply(self._metric, grad).squeeze()
        return (theta, rho)

    def _prob_of_delayed_rejection(self, hastings: float) -> float:
        """Log probability of making another proposal upon rejection.

        To reduce average cost per iteration for DRHMC, make the delayed rejections
        probabilistic, such that a subsequent proposal is not mandatory upon rejection.

        Probabilistic delayed rejection proposes a new sample with some probability
        dependent on where we are in the distribution. If a proposal has low acceptance
        probability and was rejected, propose another sample; if a proposal has high
        acceptance probablity but was rejected by chance, do not propose another
        sample. More details found in section 3.2 of Modi et al.

        A heuristic proposal probability that achieve this is eqn. 31 in Modi et al:
        the probability of making proposal k+1, after rejecting proposal k, is the
        probability of rejecting proposal k. The proposal probability of making the
        first proposal, however, is always 1.

        This proposal probability modifies the proposal kernel, which alters detailed
        balance. Thus the probability of accepting a proposal is multiplied by the
        hasting term of the propopsed sample divided by the hastings term of the
        current sample.

        Args:
            hastings: log probability of rejecting all previous proposals

        Returns:
            float: log probability of making another proposal
        """
        return hastings if self._probabilistic else 0.0

    def sample(self) -> DrawAndLogP:
        """Sample from target distribution with DrGhmcDiag sampler.

        From current sample (theta, rho), propose new sample (theta_prop, rho_prop)
        and compute its acceptance probability. If accepted, return the new sample and
        its log density; if rejected, propose a new sample and repeat. Stop when the
        maximum number of proposals is reached or when probabilistic delayed rejection
        terminates early.

        Flip momemntum after each proposal to ensure detailed balance. Flip momentum
        before return statement for momentum persistence in Generalized HMC.

        Returns:
            DrawAndLogP: Tuple of (sample, log density)
        """
        self._rho = self._rng.normal(
            loc=self._rho * np.sqrt(1 - self._dampening),
            scale=self._dampening,
            size=self._dim,
        )
        logp_cur = self.joint_logp(self._theta, self._rho)
        hastings_cur, reject_logp = 0.0, 0.0

        for k in range(self._num_proposals):
            pdr = self._prob_of_delayed_rejection(reject_logp)
            if not np.log(self._rng.uniform()) < pdr:
                break

            stepsize, steps = self._stepsize_list[k], self._steps_list[k]
            theta_prop, rho_prop = self.leapfrog(
                self._theta, self._rho, stepsize, steps
            )
            rho_prop = -rho_prop

            accept_logp, logp_prop = self.accept(
                theta_prop, rho_prop, k, hastings_cur, logp_cur
            )
            if np.log(self._rng.uniform()) < accept_logp:
                self._theta, self._rho = theta_prop, rho_prop
                logp_cur = logp_prop
                break

            reject_logp = np.log1p(-np.exp(accept_logp))
            hastings_cur += reject_logp
        self._rho = -self._rho  # always flip momentum in GHMC
        return self._theta, logp_cur

    def accept(
        self,
        theta_prop: VectorType,
        rho_prop: VectorType,
        k: int,
        hastings_cur: float,
        logp_cur: float,
    ) -> tuple[float, float]:
        """Log acceptance probability of transitioning from current to proposed sample.

        Recursively compute log probability density of accepting transition from
        current sample (theta, rho) to proposed sample (theta_prop, rho_prop).
        Information about the current sample is contained in logp_cur and is *not*
        passed into the function directly via (theta, rho) for efficient reuse.

        Acceptance probability for the k-th proposed sample, with probabilistic delayed
        rejection, is computed by extending eqns. 29, 30, and 31 of Modi et al. The
        acceptance probability contains three terms for the current and proposed
        samples respectively:

            1. the joint log density
            2. the hastings term for asymmetric proposal kernels; simplfies to the probability of rejecting previous/ghost proposals
            3. the probability of proposing another sample upon rejection

        Optimize computation by passing hastings_cur and logp from previous proposal
        and by returning logp_prop. This is less readable but ensures (1) only one
        density evaluation per sample outside of leapfrog, (2) no recursion is required
        to compute the denominator of eqn. 29, and (3) recursion on the numerator of
        eqn. 29 is done efficiently.

        Proposed samples are generated by running the leapfrog integrator from the
        *current sample*. To maintain detailed balance, 'ghost proposals' are similarly
        generated by running the leapfrog integrator from the *proposed sample*.
        (Proposed samples and ghost samples of the same order are generated with the
        same leapfrog stepsize and number of steps.)

        When acceptance probability of a ghost proposal is 1, the hastings term of the
        proposed sample is completely zeroed out. Thus early stopping is implemented to
        avoid extra computation and sidestep negative infinity log probabilities.

        Flip momentum after each ghost proposal to ensure detailed balance.

        Args:
            theta_prop: proposed position
            rho_prop: proposed momentum
            k: number of delayed rejection proposals
            hastings_cur: log probability of rejecting all previous proposals
            logp: log probability of current sample

        Returns:
            tuple[float, float]: log probability of acceptance and proposed sample
        """
        logp_prop = self.joint_logp(theta_prop, rho_prop)
        hastings_prop = 0

        for i in range(k):
            stepsize, steps = self._stepsize_list[i], self._steps_list[i]
            theta_ghost, rho_ghost = self.leapfrog(
                theta_prop, rho_prop, stepsize, steps
            )
            rho_ghost = -rho_ghost

            accept_logp, _ = self.accept(
                theta_ghost, rho_ghost, i, hastings_prop, logp_prop
            )
            if accept_logp == 0:  # early stopping to avoid -inf in np.log1p
                return -np.inf, logp_prop
            reject_logp = np.log1p(-np.exp(accept_logp))
            hastings_prop += reject_logp

        pdr_prop = self._prob_of_delayed_rejection(hastings_prop)
        pdr_cur = self._prob_of_delayed_rejection(hastings_cur)

        detailed_balance = (
            (logp_prop - logp_cur)
            + (hastings_prop - hastings_cur)
            + (pdr_prop - pdr_cur)
        )
        return min(0, detailed_balance), logp_prop
